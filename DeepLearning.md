# DeepLearning

## 基本知识

### 关键组件

#### 数据(data)

        数据由样本(example, sample)组成，每个样本由特征(feature, 或协变量(covariate))的属性构成。预测的属性中有被称为标签(label,或目标(target))的属性。当每个样本的特征类别相同，特征向量是固定长度时，该长度称为数据的维度(dimensionality)。

#### 模型(model)

        模型是一个函数，输入是特征向量，输出是预测的标签。模型的参数(parameter)是函数的内部变量，通过学习算法(learning algorithm)来确定。模型的复杂度(complexity)是模型的参数的数量。

#### 网络结构(network architecture)

        网络结构是模型的组织方式，包括模型的层数、每层的节点数、每层的连接方式等。网络结构的选择取决于数据的特性和任务的复杂度。

#### 目标函数(objective function)

        目标函数是一个评估模型预测的标签和真实标签之间的差异的函数。目标函数的值越小，模型的预测越准确，因此目标函数也被称为损失函数(loss function, 或cost function)。常见的目标函数有平方误差(squared error)、交叉熵(cross-entropy)等。目标函数一般根据模型的参数来计算，并取决于数据集。一般在被称为训练集(training set)的数据集上进行参数调优使目标函数最小化，在被称为测试集(test set)的数据集上进行验证。

#### 优化算法(optimization algorithm)

        优化算法是一种通过调整模型的参数来最小化目标函数的算法。常见的优化算法有梯度下降(gradient descent)、牛顿法(Newton's method)、拟牛顿法(quasi-Newton method)等。

#### 超参数(hyperparameter)

        超参数是模型的参数，但不是通过学习算法来确定的。超参数的选择取决于数据的特性和任务的复杂度。常见的超参数有学习率(learning rate)、正则化参数(regularization parameter)、网络结构等。超参数一般为手动进行设置，用于控制模型的行为与性能。

### 常见机器学习模型

#### 监督学习(supervised learning)

        监督学习擅长在“给定输入特征”的情况下预测标签。每个“特征-标签”对都是一个样本。监督学习的过程一般分为三个步骤：1.从已知大数据样本随机选取子集，为每个样本获取真实标签；2.选择有监督的学习算法，输入训练数据集，输出“完成学习”的模型；3.使用模型预测新的数据样本的标签。

##### 回归(regression)

        回归是一种监督学习，指的是预测连续值的标签，比如预测房价、电影评分等。回归的目标函数一般是平方误差(squared error)。

##### 分类(classification)

        分类是一种监督学习，指的是预测离散值的标签，比如分辨垃圾邮件、猫狗识别等。分类的目标函数一般是交叉熵(cross-entropy)。

##### 实际应用

        常见的监督学习应用有：标记问题、搜索引擎排序、推荐系统等。
        监督学习有时还需考虑序列问题，需要记住之前的输入，比如语音识别、视频分析等。

#### 无监督学习(unsupervised learning)

        无监督学习擅长在“没有标签”的情况下学习数据的结构。无监督学习的常见分类有：聚类(clustering, 指的是将数据分成不同的组)、主成分分析(principal component analysis, 通过少量的参数来描述数据)、因果关系(causal relationship, 指的是找到数据之间的因果关系)和概率图模型(probabilistic graphical model, 指的是用图来表示数据的概率分布)等。

#### 强化学习(reinforcement learning)

        强化学习是一种学习如何在环境中采取行动以获得最大奖励的方法。强化学习的目标是找到一个策略(policy)，使得在每个状态下采取的行动能够最大化长期奖励。强化学习的常见应用有：游戏、机器人控制、自然语言处理等。
        强化学习需要处理部分可观测性问题，当环境可被完全观察到时，称为马尔可夫决策过程(Markov decision process)；当状态不依赖于之前的操作时，称为上下文赌博机(contextual bandit)；当没有状态，只有最初未知的奖励时，称为多臂赌博机(multi-armed bandit)。

### PyTorch 基础

#### 初始化

-   张量(tensor)创建: `torch.tensor(data)`(创建张量)、`torch.zeros(shape)`(创建全 0 张量)、`torch.ones(shape)`(创建全 1 张量)、`torch.rand(shape)`(创建随机张量)
-   张量属性: `tensor.shape`(张量的形状)、`tensor.dtype`(张量的数据类型)、`tensor.grad`(张量的梯度)
-   张量操作: `tensor.reshape()`(改变张量的形状)、`tensor.backward()`(计算张量的梯度)、`tensor.item()`(返回张量的标量值)、`tensor.numpy()`(将张量转换为 numpy 数组)、`torch.sort()`(沿指定维度排序张量)、`torch.mean()`(沿指定维度计算张量的均值)、`torch.sum()`(沿指定维度计算张量的和)、`torch.max()`(沿指定维度取张量的最大值)，`torch.cat()`(沿指定维度拼接张量)

#### 常见机制

-   广播机制: 当两个张量的形状不同时，PyTorch 会自动扩展维度，复制张量使得两个张量的形状相同，然后进行运算。广播机制一般只应用于长度为 1 的维度。
-   索引和切片: 与 Python 列表类似，不赘述。
-   内存节省: 为了避免计算时对象的重新创建，需要使用变量执行原地操作`X[:] = X + Y`。

#### 数据预处理

数据预处理一般使用 panads 库，因为 pandas 底层与张量兼容。

-   数据读取: `pd.read_csv()`(读取 csv 文件)、`pd.read_excel()`(读取 excel 文件)
-   处理缺失值: 常见插值法和删除法。一般用到以下函数：`df.dropna()`(删除缺失值)、`df.fillna()`(填充缺失值)、`df.mean()`(计算均值)、`df.median()`(计算中位数)、`df.mode()`(计算众数)、`pd.get_dummies()`(独热编码, 将类别变量转换为数值变量)。
-   转为张量: `torch.tensor(pd.to_numpy(df))`。通过 numpy 作为中间桥梁，将 pandas 数据转换为张量。

#### 线性代数

-   标量(scalar): 仅包含一个数值称为标量。
-   向量(vector): 一个维度的数组称为向量，可以视为列表。$x \in \mathbb{R}^n$。
-   矩阵(matrix): 二维数组称为矩阵。$A \in \mathbb{R}^{m \times n}$。
-   张量(tensor): 多维数组称为张量。
-   张量的算法基本性质: 同样形状的张量可以相加、相减、相乘、相除。其中相乘又称为 Hadamard 积，数学符号为$\odot$。
-   降维: 张量的降维是指将张量的维度降低。降维的方法有两种：1.张量的求和, `torch.sum()`；2.张量的平均值, `torch.mean()`。两者可设置`axis`参数，表示从外向内的维度进行运算。当然，它们可以通过设置`keepdim`参数来保持维度, 配合广播机制，可以实现快速计算。
-   范数(norm): 向量的模从二维、三维推广到 n 维，称为范数，数学符号为$\|x\|_p$。范数的公式: $\|x\|_p = (\sum_{i=1}^n |x_i|^p)^{1/p}$。一般$\mathbf{L}_2$会省略。而在矩阵中，存在 Frobenius 范数，即矩阵的所有元素的平方和再开方，数学符号为$\|A\|_F$。python 中使用`torch.norm()`计算所有范数。

#### 自动微分

-   自动微分: PyTorch 的自动微分机制是通过`torch.autograd`实现的。自动微分的核心是`torch.Tensor`类的`grad`属性，该属性保存了张量的梯度。当 B 张量进行了相应的操作赋值给 A，再让 A 调用反向传播算法`tensor.backward()`，B 的梯度就会被计算并保存在 B 的`grad`属性中。
-   分离计算: 有时候我们不希望某个张量的梯度被计算，可以使用`tensor.detach()`方法。该方法会返回一个新的张量，新的张量的`requires_grad`属性为 False, 它将在计算图中作为常量处理。
-   小技巧: 可以用`sum()`方法将张量的所有元素求和，再调用`backward()`方法，这样可以计算出每个样本单独计算的偏导数和，而不是计算微分矩阵(雅各比矩阵)。

#### 概率

-   基本概率论: PyTorch 中的概率分布函数在`torch.distributions`模块中。常见的概率分布有：均匀分布(Uniform)、正态分布(Normal)、伯努利分布(Bernoulli)、多项式分布(Multinomial)、泊松分布(Poisson)等。通过概率分布传入参数，包括 total_count(抽取次数)、probs(概率，用向量表示)等，再用`sample()`方法抽取样本。

#### 文档查阅

-   查找模块的函数与类: `print(dir(***))`
-   查找函数与类的用法: `help(***.***)`

## 线性神经网络

### 线性回归

-   公式: $\hat{y} = w^Tx + b$
-   目标: y
-   特征: x
-   权重: w
-   偏置, 截距, 偏移: b
-   损失函数: 平方误差, $L(w, b) = \frac{1}{2n} \sum_{i=1}^n (y_i - \hat{y}_i)^2=\frac{1}{2n} \sum_{i=1}^n (y_i - w^Tx_i - b)^2$

### 解与梯度下降

-   解析解: 指的是通过求导得到的解，$w = (X^TX)^{-1}X^Ty$
-   梯度下降: 不是所有问题都像线性回归那样有解析解，梯度下降是一种通过迭代来逼近解的方法。梯度下降的思想通俗来说，像是在山间选择不同的路径走到山底。而寻找不同走到山地的方法就是梯度下降的不同变种。梯度下降法可以用每次采取小批量样本进行梯度更新，就比如随机抽样小批量$\beta$，则可以用$(w, b) \leftarrow (w, b) - \frac{\eta}{|\beta|} \sum_{i \in \beta} \nabla_{(w, b)} L(w, b)$来更新参数。其中$\eta$是学习率，即表示每次更新的步长，如果太短，则会导致走的太慢，如果太长，则会导致走过头。
-   泛化: 泛化是指在模型中寻找一组参数，不仅使模型在训练集上表现良好，还能在测试集上表现良好。泛化能力是模型的重要性能指标。

### 常见 API

-   `torch.utils.data` 介绍——`data.TensorDataset`: 该类是 PyTorch 中的数据集类，用于将数据集和标签组合在一起。`data.DataLoader`: 该类是 PyTorch 中的数据加载类，用于将数据集分批次加载。
-   `torch.nn` 介绍——`nn.Sequential`: 该类是 PyTorch 中的模型容器类，用于将多个模型组合在一起。`nn.Linear`: 该类是 PyTorch 中的线性层类，用于定义线性模型，其中`in_features`表示输入特征的数量，`out_features`表示输出特征的数量,`weight`定义权重，`bias`定义偏置。`nn.MSELoss`: 该类是 PyTorch 中的均方误差损失类，用于定义损失函数。`nn.optim.SGD`: 该类是 PyTorch 中的随机梯度下降类，用于定义优化器。`nn.Flatten`: 该类是 PyTorch 中的展平层类，用于将多维张量展平为一维张量。
-   `torch.optim` 介绍——`optim.SGD`: 该类是 PyTorch 中的随机梯度下降类，用于定义优化器。`optim.Adam`: 该类是 PyTorch 中的 Adam 优化器类，用于定义优化器。其中，`step()`函数用于更新参数，`zero_grad()`函数用于梯度清零。

### softmax 回归

-   softmax 回归指的是多分类问题的线性回归，其输出是一个概率分布。softmax 可以将未归一化的预测值转换为概率分布，即预测变换为非负数且和为 1 的概率，同时保证模型的可导性。
-   校准(Calibration): 校准是指模型的预测概率与真实概率之间的关系。如果模型的预测概率与真实概率一致，则称为校准。校准是模型的重要性能指标。
-   损失函数: 采用最大似然估计，由于传统规定越小越佳，所以最大似然取负数，$l(y, \hat{y}) = -\sum_{i=1}^n y_i \log \hat{y}_i$。$y$是真实标签，$\hat{y}$是预测标签。其中，该函数又称为交叉熵损失函数。
-   softmax 数学公式为：$\hat{y}_i = \frac{o_j}{\sum_{j=1}^n \exp(o_j)}$。其中，$o_j = x^Tw_j + b_j$。则其损失函数对$o_j$的梯度为：$\frac{\partial l}{\partial o_j} = softmax(o)_j - y_j$。

### 信息论基础

-   熵: 熵是信息的期望值，用于衡量信息的不确定性。熵越大，信息的不确定性越大。熵的公式为：$H(X) = -\sum_{i=1}^n p(x_i) \log p(x_i)$。其中，$p(x_i)$是事件$x_i$发生的概率。
-   交叉熵: 交叉熵除了表示“最大观测数据的似然”，还可以表示“最小化传达标签所需的惊异”。

### 图像分类数据集

-   Fashion-MNIST 数据集: Fashion-MNIST 数据集是一个包含 10 个类别的图像数据集，每个类别包含 6000 张训练图像和 1000 张测试图像。
-   数据读取: 通过`torch.utils.data`模块中的`DataLoader`类，可以将数据集分批次加载。通过`torchvision`模块中的`datasets`类，可以加载 Fashion-MNIST 数据集。通过`torchvision`模块中的`transforms`类，可以对数据集进行预处理。

## 多层感知机(Multilayer Perceptron, MLP)

### 隐藏层

        线性意味着单调假设：任何特征的增大都会导致模型预测的增大或减小。但是，这个假设并不总是正确的。例如，如果一个特征与标签之间的关系是非线性的，比如像素和像素周围的上下文之间的关系（即周围像素的值），那么线性模型就会出现问题。为了解决这个问题，我们可以引入一个或多个隐藏层，这些隐藏层中的每一个都包含了一个非线性的激活函数，这样就可以让模型学习到特征之间的复杂关系。

### 激活函数

        激活函数是神经网络中的一个重要组件。

-   ReLu 函数：ReLU 函数是一个非线性函数，它在输入大于 0 时返回输入，否则返回 0。ReLU 函数的公式为：$ReLU(x) = max(x, 0)$。
-   Sigmoid 函数：Sigmoid 函数是一个非线性函数，它将输入映射到 0 和 1 之间。Sigmoid 函数的公式为：$sigmoid(x) = \frac{1}{1 + \exp(-x)}$。
-   Tanh 函数：Tanh 函数是一个非线性函数，它将输入映射到-1 和 1 之间。Tanh 函数的公式为：$tanh(x) = \frac{\exp(x) - \exp(-x)}{\exp(x) + \exp(-x)}$。

### 模型选择

-   验证集: 选择模型的时候需要用到验证集，验证集是用来调整模型超参数的。在训练集上训练模型，在验证集上调整模型超参数，最后在测试集上评估模型性能。
-   欠拟合和过拟合: 当训练误差和验证误差都很严重，但是它们差距很小时，说明数据欠拟合，此时需要更复杂的模型；当训练误差明显低于验证误差时，此时表明数据过拟合，需要调整验证误差，可以使用权重衰减优化。参数的多少可能会影响拟合程度。
-   K 折交叉验证: 交叉验证是一种评估模型性能的方法，它将数据集分为 k 个子集，然后将模型训练 k 次，每次使用 k-1 个子集作为训练集，剩下的一个子集作为验证集。最后将 k 次的验证结果取平均值作为最终的验证结果。

### 权重衰减

-   正则化: 正则化即通过减小目标的数据扰动来减小模型的复杂度。正则化的目的是防止过拟合。正则化的方法有很多种，比如权重衰减、Dropout、数据增强等。
-   权重衰减: 权重衰减是一种正则化方法，它通过向损失函数添加一个惩罚项来限制权重的大小。权重衰减的公式为：$L(w, b) = L_0 + \frac{\lambda}{2} \sum_{i=1}^n ||w_i||^2$。其中，$\lambda$是权重衰减的超参数，$||w_i||^2$是权重的 L2 范数, $L_0$是原始的损失函数。
-   pytorch 中的权重衰减: 在 PyTorch 中，可以通过在优化器中设置`weight_decay`参数来实现权重衰减。

### 暂退法

-   